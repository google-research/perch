{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfDwaWNTZCZ"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This is an end-to-end demo of using Perch Hoplite to create a custom classifier using pre-trained embeddings and searching for a particular sound, focused on marine passive acoustic data and whale vocalization classification. For this demo, we are using all publicly available data from NOAA and NCEI's [Passive Acoustic Monitoring Data Archive](https://console.cloud.google.com/marketplace/details/noaa-public/passive_acoustic_monitoring?pli=1) hosted on Google Cloud. This tutorial uses the newer perch-hoplite infrastructure, but the earlier version with additional explanations based on the prior codebase (now out of date) can be found in the NeurIPS [2023 Climate Change AI Tutorial](https://colab.research.google.com/github/climatechange-ai-tutorials/bioacoustic-monitoring/blob/main/Agile_Modeling_for_Bioacoustic_Monitoring.ipynb).\n",
        "\n",
        "\n",
        "## Note on Agile Modeling process\n",
        "This notebook is a combination and more detailed end-to-end version of the two agile modeling notebooks in perch-hoplite. The first notebook [1_embed_audio_v2.ipynb](https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite/agile/1_embed_audio_v2.ipy) is the first step in the agile modeling process where we create a database of embeddings of the audio we wish to search given a pre-trained model. We then would use the step 2 colab notebook ([2_agile_modeling_v2.ipynb](https://github.com/google-research/perch-hoplite/blob/main/perch_hoplite/agile/2_agile_modeling_v2.ipynb)) to search and classify that database for a particular call type. You can read more about the agile modeling process in [this paper](https://arxiv.org/abs/2505.03071).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci24GEJQH854"
      },
      "source": [
        "\n",
        "## Set-up notes\n",
        "For this demo, we recommend using a hosted Google Colab runtime and saving the output via Google Drive. To load this notebook in Google Colab - go to https://colab.research.google.com/. Under \"Open Notebook\" - go to the tab for \"GitHub\" and paste the url for this notebook there.\n",
        "\n",
        "To connect to a hosted runtime, in the top right corner, click \"Connect.\"\n",
        "Then select \"Connect to a hosted runtime.\" You should be automatically connected to a colab runtime. We recommend that you also select a GPU runtime - if you click \"Change runtime type\" you can confirm or switch the runtime type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Db84ySxSJYA"
      },
      "source": [
        "## [Optional] perch-hoplite installation for hosted runtimes\n",
        "\n",
        "If you have not already installed perch-hoplite (particularly if you are using a hosted Colab runtime), make sure to install perch-hoplite from the Github source to ensure the most recent version is installed. After installation, you will need to restart your runtime before running anything else. Go to the top menu, select \"Runtime\" then \"Restart Session\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7bUZkS_Rawd"
      },
      "outputs": [],
      "source": [
        "#@title Only run this code if you need to install perch-hoplite\n",
        "#@markdown You will likely be asked to restart your runtime, but after restarting, don't need to rerun this block.\n",
        "!pip install git+https://github.com/google-research/perch-hoplite.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTtVnkC-6_i7"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "from etils import epath\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "from perch_hoplite.agile import colab_utils\n",
        "from perch_hoplite.agile import embed\n",
        "from perch_hoplite.agile import source_info\n",
        "from perch_hoplite.db import brutalism\n",
        "from perch_hoplite.db import interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGlTTBhumFsB"
      },
      "source": [
        "## For saving data in this example, we're going to use Google Drive.\n",
        "\n",
        "This example is assuming you are running colab from a hosted runtime - this means the code is running on a cloud service not on your local machine, and thus won't have access to your local file directory. The raw audio files are coming from a public URL, so we will need a place to save the embeddings database and any results. While you can save all these created files to a temporary folder (e.g. '/tmp/...'), if your runtime crashes, you will potentially lose those files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJKrGUdAMG-Q"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive for saving data\n",
        "#@markdown When you run this, you will be prompted to authenticate to grant permissions for colab to save to your Google Drive.\n",
        "\n",
        "#@markdown After you grant permissions, you will see a code that you will need to copy and paste in the form output that will be generated below.\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJYSvhnanrMG"
      },
      "outputs": [],
      "source": [
        "#@title Create a new folder in Drive (if it doesn't already exist) within your Google drive.\n",
        "base_dir = '/content/drive/My Drive/'\n",
        "#@ markdown Name of your new folder in Drive\n",
        "new_folder_name = 'noaa_demo' #@param\n",
        "\n",
        "drive_output_directory = base_dir + new_folder_name\n",
        "\n",
        "try:\n",
        "  if not os.path.exists(drive_output_directory):\n",
        "    os.makedirs(drive_output_directory, exist_ok=True)\n",
        "    print(f'Directory {drive_output_directory} created successfully.')\n",
        "  else:\n",
        "    print(f'Directory {drive_output_directory} already exists.')\n",
        "except OSError as e:\n",
        "    print(\"Error:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T4vILrO80iP"
      },
      "source": [
        "# Embed the audio data\n",
        "\n",
        "## Example data - NOAA PIFSC Saipan selection for Bryde's whale biotwangs\n",
        "For this example, we are loading audio data from the [Passive Acoustic Data](https://www.ncei.noaa.gov/products/passive-acoustic-data) archive by NOAA and NCEI, where the files are stored on a [Google Cloud Bucket](https://console.cloud.google.com/marketplace/details/noaa-public/passive_acoustic_monitoring?pli=1). Note that the files are stored as .flac files, which needs to be uncompressed to access the full metadata associated with the .wav files in order to have correct time stamps (see the [README](https://storage.googleapis.com/noaa-passive-bioacoustic/pifsc/README.md)). Because these files are very large (generally over 8 hours of audio and over 1 GB each), we do need to shard the files for embedding. The files are all accessible via a public url, so if applying this on a different set of data, you'll need to specify a different pathway where the data are stored.\n",
        "\n",
        "The file chosen for this example is one that was annotated and labeled specifically for the Bryde's whale biotwang and had a large number of detections of this particular sound found (see [paper](https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2024.1394695/full)). However, you can repeat this process or change out the file to try a new selection of audio.\n",
        "\n",
        "## Starting model choices\n",
        "Given the [multispecies whale model](https://www.kaggle.com/models/google/multispecies-whale) is pre-trained to detect Bryde's whale biotwangs (see [blog post for more details](https://research.google/blog/whistles-songs-boings-and-biotwangs-recognizing-whale-vocalizations-with-ai)), this is a good starting point for a pre-trained model. You can also select other models - [SurfPerch](https://www.kaggle.com/models/google/surfperch) would be another solid option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6zdGxl68vft"
      },
      "outputs": [],
      "source": [
        "# @title Configuration { vertical-output: true }\n",
        "\n",
        "# @markdown Configure the raw dataset and output location(s).  The format is a mapping from\n",
        "# @markdown a dataset_name to a (base_path, fileglob) pair.  Note that the file\n",
        "# @markdown globs are case sensitive.  The dataset name can be anything you want.\n",
        "#\n",
        "# @markdown This structure allows you to move your data around without having to\n",
        "# @markdown re-embed the dataset.  The generated embedding database will be\n",
        "# @markdown placed in the base path. This allows you to simply swap out\n",
        "# @markdown the base path here if you ever move your dataset.\n",
        "\n",
        "# @markdown By default we only process one dataset at a time.  Re-run this entire portion [Embed] of the notebook\n",
        "# @markdown once per dataset.\n",
        "\n",
        "# @markdown For example, we might set dataset_base_path to '/home/me/myproject',\n",
        "# @markdown and use the glob '\\*/\\*.wav' if all of the audio files have filepaths\n",
        "# @markdown like '/home/me/myproject/site_XYZ/audio_ABC.wav' (e.g. audio files are contained in subfolders of the base directory).\n",
        "\n",
        "\n",
        "# @markdown 1. Create a unique name for the database that will store the embeddings for the target data.\n",
        "# @markdown For this example, we use the name of the large audio file, but you can use a different name here.\n",
        "dataset_name = 'Saipan_A_06_151006_091215'  # @param {type:'string'}\n",
        "# @markdown 2. Input the filepath for the folder that is containing the input audio files.\n",
        "dataset_base_path = 'gs://noaa-passive-bioacoustic/pifsc/audio/pipan/saipan/pipan_saipan_06/audio'  #@param {type:'string'}\n",
        "# @markdown 3. Input the file pattern for the audio files within that folder that you want to embed. Some examples for how to input:\n",
        "# @markdown - All files in the base directory of a specific type (not subdirectories): e.g. `*.wav` (or `*.flac` etc) will generate embeddings for all .wav files (or whichever format) in the dataset_base_path\n",
        "# @markdown - All files in one level of subdirectories within the base directory: `*/*.flac` will generate embeddings for all .flac files\n",
        "# @markdown - Single file: `myfile.wav` will only embed the audio from that specific file.\n",
        "dataset_fileglob = 'Saipan_A_06_151006_091215.df20.*.flac'  # @param {type:'string'}\n",
        "\n",
        "# @markdown 4. [Optional] If saving the embeddings database to a new directory, specify here.\n",
        "# @markdown Otherwise, leave blank - by default the embeddings database output will be saved within\n",
        "# @markdown dataset_base_path where the audio is located. You do not need to specify db_path unless you want to maintain multiple\n",
        "# @markdown distinct embedding databases, or if you would like to save the output\n",
        "# @markdown in a different folder. If your input audio data is accessed\n",
        "# @markdown from a public URL, we recommend specifying a separate output directory here.\n",
        "db_subdir = '/agile_Saipan_A_06_151006_091215'  # @param {type:'string'}\n",
        "db_path = drive_output_directory + db_subdir if db_subdir else None\n",
        "if not db_path or db_path == 'None':\n",
        "  db_path = None\n",
        "\n",
        "\n",
        "# @markdown 5. Choose a supported model to generate embeddings: `perch_8` or `birdnet_v2.3` are most common\n",
        "# @markdown for birds. Other choices include `surfperch` for coral reefs or\n",
        "# @markdown `multispecies_whale` for marine mammals.\n",
        "model_choice = 'surfperch'  #@param['perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n",
        "\n",
        "# @markdown 6. [Optional] Shard the audio for embeddings. File sharding automatically splits audio files into smaller chunks\n",
        "# @markdown for creating embeddings. This limits both system and GPU memory usage,\n",
        "# @markdown especially useful when working with long files (\u003e1 hour).\n",
        "use_file_sharding = True  # @param {type:'boolean'}\n",
        "# @markdown If you want to change the length in seconds for the shards, specify here.\n",
        "shard_length_in_seconds = 75  # @param {type:'number'}\n",
        "\n",
        "# @markdown We also need to specify the targeted sample rate. -2 will give the target sample rate of the model,\n",
        "# @markdown -1 will use the target sample rate of the original source audio, and any other number \u003e0 will\n",
        "# @markdown use that specified rate.\n",
        "target_sample_rate_hz = -1  # @param {type:'number'}\n",
        "\n",
        "audio_glob = source_info.AudioSourceConfig(\n",
        "    dataset_name=dataset_name,\n",
        "    base_path=dataset_base_path,\n",
        "    file_glob=dataset_fileglob,\n",
        "    min_audio_len_s=1.0,\n",
        "    target_sample_rate_hz=target_sample_rate_hz,\n",
        "    shard_len_s=float(shard_length_in_seconds) if use_file_sharding else None,\n",
        ")\n",
        "\n",
        "configs = colab_utils.load_configs(\n",
        "    source_info.AudioSources((audio_glob,)),\n",
        "    db_path,\n",
        "    model_config_key=model_choice,\n",
        "    db_key='sqlite_usearch',\n",
        ")\n",
        "configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN9Uyy1yqAWS"
      },
      "outputs": [],
      "source": [
        "#@title Initialize the hoplite database (DB) { vertical-output: true }\n",
        "global db\n",
        "db = configs.db_config.load_db()\n",
        "num_embeddings = db.count_embeddings()\n",
        "\n",
        "print('Initialized DB located at ', configs.db_config.db_config.db_path)\n",
        "\n",
        "def drop_and_reload_db(_) -\u003e interface.HopliteDBInterface:\n",
        "  db_path = epath.Path(configs.db_config.db_config.db_path)\n",
        "  for fp in db_path.glob('hoplite.sqlite*'):\n",
        "    fp.unlink()\n",
        "  (db_path / 'usearch.index').unlink()\n",
        "  print('\\n Deleted previous db at: ', configs.db_config.db_config.db_path)\n",
        "  db = configs.db_config.load_db()\n",
        "\n",
        "#@markdown If `drop_existing_db` set to True, when the database already exists and contains embeddings,\n",
        "#@markdown then those existing embeddings will be erased. You will be prompted to confirm you wish to delete those existing\n",
        "#@markdown embeddings. If you want to keep existing embeddings in the database, then set to False, which will append the new\n",
        "#@markdown embeddings to the database.\n",
        "drop_existing_db = False  #@param {type:'boolean'}\n",
        "\n",
        "if num_embeddings \u003e 0 and drop_existing_db:\n",
        "  print('Existing DB contains datasets: ', db.get_dataset_names())\n",
        "  print('num embeddings: ', num_embeddings)\n",
        "  print('\\n\\nClick the button below to confirm you really want to drop the database at ')\n",
        "  print(f'{configs.db_config.db_config.db_path}\\n')\n",
        "  print(f'This will permanently delete all {num_embeddings} embeddings from the existing database.\\n')\n",
        "  print('If you do NOT want to delete this data, set `drop_existing_db` above to `False` and re-run this cell.\\n')\n",
        "\n",
        "  button = widgets.Button(description='Delete database?')\n",
        "  button.on_click(drop_and_reload_db)\n",
        "  display(button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnGWbhc0LhiU"
      },
      "outputs": [],
      "source": [
        "#@title Run the embedding { vertical-output: true }\n",
        "#@markdown This may take approximately 15 minutes to run.\n",
        "\n",
        "print(f'Embedding dataset: {audio_glob.dataset_name}')\n",
        "\n",
        "worker = embed.EmbedWorker(\n",
        "    audio_sources=configs.audio_sources_config,\n",
        "    db=db,\n",
        "    model_config=configs.model_config)\n",
        "\n",
        "worker.process_all(target_dataset_name=audio_glob.dataset_name)\n",
        "\n",
        "print('\\n\\nEmbedding complete, total embeddings: ', db.count_embeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZrzlLVtqAHY"
      },
      "source": [
        "If you already have a database saved from running this earlier, you may get the following error running the above cell when trying to add more embeddings to the database:\n",
        "\n",
        "```\n",
        "AssertionError: The configured model key does not match the model key that is already in the DB.\n",
        "```\n",
        "For a given database name and saved location, the same embedding model and settings must be used. If you want to use a different embedding model, then you'll need to create a new database and save location in the \"Configuration\" cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvVuFw-somHe"
      },
      "outputs": [],
      "source": [
        "#@title Per dataset statistics { vertical-output: true }\n",
        "#@markdown This tells us how many unique segments are embedded in the database.\n",
        "\n",
        "for dataset in db.get_dataset_names():\n",
        "  print(f'\\nDataset \\'{dataset}\\':')\n",
        "  print('\\tnum embeddings: ', db.get_embeddings_by_source(dataset, source_id=None).shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ihBNRbwuuwal"
      },
      "outputs": [],
      "source": [
        "#@title Show example embedding search\n",
        "#@markdown As an example (and to show that the embedding process worked), this\n",
        "#@markdown selects a single embedding from the database and outputs the embedding ids of the\n",
        "#@markdown top-K (k = 128) nearest neighbors in the database.\n",
        "\n",
        "q = db.get_embedding(db.get_one_embedding_id())\n",
        "%time results, scores = brutalism.brute_search(worker.db, query_embedding=q, search_list_size=128, score_fn=np.dot)\n",
        "print([int(r.embedding_id) for r in results])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGs2vo60dnjj"
      },
      "source": [
        "# Agile Modeling - Search and classify\n",
        "\n",
        "For this example, we are going to search for \"Biotwang\" calls produced by Bryde's whales. The selection of audio above was chosen because of a large number of detections of Biotwangs over the duration of the audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOER761Sdo8o"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from perch_hoplite.agile import audio_loader\n",
        "from perch_hoplite.agile import classifier\n",
        "from perch_hoplite.agile import classifier_data\n",
        "from perch_hoplite.agile import embedding_display\n",
        "from perch_hoplite.agile import source_info\n",
        "from perch_hoplite.db  import brutalism\n",
        "from perch_hoplite.db import score_functions\n",
        "from perch_hoplite.db  import search_results\n",
        "from perch_hoplite.db import sqlite_usearch_impl\n",
        "from perch_hoplite.zoo import model_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJn0vuG2dy2v"
      },
      "outputs": [],
      "source": [
        "#@title Load model and connect to database. { vertical-output: true }\n",
        "\n",
        "#@markdown Location of database containing audio embeddings - if you are running this\n",
        "#@markdown in the same session as the embeddings (e.g. you haven't had to restart your runtime),\n",
        "#@markdown then you can leave this blank and it will fill in with the same db_path\n",
        "#@markdown as the embeddings defined above. However, you can fill out the path\n",
        "#@markdown if you are running this in a new session or want to load a different saved database.\n",
        "load_db_path = ''  #@param {type:'string'}\n",
        "if load_db_path is None:\n",
        "  load_db_path = db_path\n",
        "#@markdown Identifier (eg, the annotator's name or unique ID) to attach to labels produced during validation.\n",
        "annotator_id = 'laurenharrell'  #@param {type:'string'}\n",
        "#@markdown Sample rate for loading audio - for the NOAA raw data this is 10_000,\n",
        "#@markdown but note that the model sample rates will be different from this rate.\n",
        "#@markdown If left blank, then the sample rate will be input from the model's\n",
        "#@markdown sample rate.\n",
        "audio_loader_sample_rate_hz = 10_000  #@param {type:'number'}\n",
        "\n",
        "db = sqlite_usearch_impl.SQLiteUsearchDB.create(db_path)\n",
        "db_model_config = db.get_metadata('model_config')\n",
        "embed_config = db.get_metadata('audio_sources')\n",
        "model_class = model_configs.get_model_class(db_model_config.model_key)\n",
        "embedding_model = model_class.from_config(db_model_config.model_config)\n",
        "audio_sources = source_info.AudioSources.from_config_dict(embed_config)\n",
        "\n",
        "if audio_loader_sample_rate_hz == None:\n",
        "  audio_loader_sample_rate_hz = embedding_model.sample_rate\n",
        "\n",
        "if hasattr(embedding_model, 'window_size_s'):\n",
        "  window_size_s = embedding_model.window_size_s\n",
        "else:\n",
        "  window_size_s = 5.0\n",
        "audio_filepath_loader = audio_loader.make_filepath_loader(\n",
        "    audio_sources=audio_sources,\n",
        "    window_size_s=window_size_s,\n",
        "    sample_rate_hz=audio_loader_sample_rate_hz,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lny3TqnoeK-Z"
      },
      "outputs": [],
      "source": [
        "#@title Load query audio. { vertical-output: true }\n",
        "\n",
        "#@markdown The `query_uri` can be a URL, filepath, or Xeno-Canto ID\n",
        "#@markdown (like `xc777802`, containing an Eastern Whipbird (`easwhi1`)).\n",
        "#@markdown We have a few pre-selected examples of Bryde's whale biotwang in a\n",
        "#@markdown public folder on Google cloud, you can change the example by replacing\n",
        "#@markdown the number 3 with any digit between 1 and 5.\n",
        "query_uri = 'gs://bioacoustics-www1/multispecies_blog_media/Be_example3.wav'  #@param {type:'string'}\n",
        "query_label = 'Be_biotwang'  #@param {type:'string'}\n",
        "\n",
        "\n",
        "query = embedding_display.QueryDisplay(\n",
        "    uri=query_uri, offset_s=0.0, window_size_s=5.0)\n",
        "_ = query.display_interactive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVdLJJd9gnjo"
      },
      "source": [
        "# Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHUJ_NwQWZNB"
      },
      "outputs": [],
      "source": [
        "#@title Embed the Query and Search. { vertical-output: true }\n",
        "\n",
        "#@markdown Number of results to find and display.\n",
        "num_results = 100  #@param\n",
        "query_embedding = embedding_model.embed(\n",
        "    query.get_audio_window()).embeddings[0, 0]\n",
        "\n",
        "#@markdown If checked, search for examples\n",
        "#@markdown near a particular target score.\n",
        "target_sampling = False  #@param {type: 'boolean'}\n",
        "\n",
        "#@markdown When target sampling, target this score.\n",
        "target_score = -1.0  #@param\n",
        "if not target_sampling:\n",
        "  target_score = None\n",
        "\n",
        "#@markdown If True, search the full DB. Otherwise, use approximate\n",
        "#@markdown nearest-neighbor search.\n",
        "exact_search = True  #@param {type: 'boolean'}\n",
        "\n",
        "score_function_name = 'dot' #@param['neg_euclidean','dot','cos']\n",
        "\n",
        "if exact_search:\n",
        "  score_fn = score_functions.get_score_fn(score_function_name, target_score=target_score)\n",
        "  results, all_scores = brutalism.threaded_brute_search(\n",
        "      db, query_embedding, num_results, score_fn=score_fn)\n",
        "  # TODO(tomdenton): Better histogram when target sampling.\n",
        "  _ = plt.hist(all_scores, bins=100)\n",
        "  hit_scores = [r.sort_score for r in results.search_results]\n",
        "  plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
        "              color='r', alpha=0.5)\n",
        "else:\n",
        "  ann_matches = db.ui.search(query_embedding, count=num_results)\n",
        "  results = search_results.TopKSearchResults(top_k=num_results)\n",
        "  for k, d in zip(ann_matches.keys, ann_matches.distances):\n",
        "    results.update(search_results.SearchResult(k, d))\n",
        "\n",
        "#@markdown Note: the query results will always be ordered where higher values indicate a stronger match for the target query/target score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y21fWjEwXj68"
      },
      "outputs": [],
      "source": [
        "#@title Display Results. { vertical-output: true }\n",
        "\n",
        "#@markdown Click on the button once for a positive label (will turn green), and\n",
        "#@markdown a second click on the same button will change the color to orange indicating\n",
        "#@markdown a negative label. If you want to undo a label - then click a third time to return the button to\n",
        "#@markdown a default background and will be assumed unlabeled.\n",
        "\n",
        "display_results = embedding_display.EmbeddingDisplayGroup.from_search_results(\n",
        "    results, db, sample_rate_hz=audio_loader_sample_rate_hz, frame_rate=100,\n",
        "    audio_loader=audio_filepath_loader)\n",
        "display_results.display(positive_labels=[query_label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3sIkOqlXzKB"
      },
      "outputs": [],
      "source": [
        "#@title Save data labels. { vertical-output: true }\n",
        "#@markdown Counts new labels added to the database.\n",
        "\n",
        "prev_lbls, new_lbls = 0, 0\n",
        "for lbl in display_results.harvest_labels(annotator_id):\n",
        "  check = db.insert_label(lbl, skip_duplicates=True)\n",
        "  new_lbls += check\n",
        "  prev_lbls += (1 - check)\n",
        "print('\\nNew labels added: ', new_lbls)\n",
        "print('\\nLabeled query results that already existed: ', prev_lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouMfqh0KnZS4"
      },
      "outputs": [],
      "source": [
        "#@title Check how many labels of each class exist in the data\n",
        "print('\\nTotal positive labels per class: ', db.get_class_counts())\n",
        "print('\\nTotal negative labels per class: ', db.get_class_counts(label_type = interface.LabelType.NEGATIVE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stA6jCjCoICg"
      },
      "source": [
        "## If you don't have enough examples to start a classfier, try a different query!\n",
        "If you have fewer than ~10-20 examples of your target class after saving the labels you just created, then try altering your query with either a new target example or playing around with a new target score or exact matching. You can repeat the above search steps as many times as you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o65wpjvyYft-"
      },
      "source": [
        "# Classify\n",
        "Once you think you have sufficient examples to try to train a model (hopefully at least 5), you can now try training a classifier on the data you embedded. Note that if you don't have any explicit negative labels, then the model assumes unlabeled data is \"weak\" negative labels, and the computation of metrics may have some errors. This won't affect your ability to keep iterating on the model, just measuring its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtsJkgcPYg6z"
      },
      "outputs": [],
      "source": [
        "#@title Classifier training. { vertical-output: true }\n",
        "\n",
        "#@markdown Set of labels to classify. If None, auto-populated from the DB.\n",
        "target_labels = None  #@param\n",
        "\n",
        "#@markdown Classifier traning hyperparams. These should not require tuning.\n",
        "learning_rate = 1e-3  #@param\n",
        "weak_neg_weight = 0.05  #@param\n",
        "l2_mu = 0.000  #@param\n",
        "num_steps = 128  #@param\n",
        "\n",
        "train_ratio = 0.9  #@param\n",
        "batch_size = 128  #@param\n",
        "weak_negatives_batch_size = 128  #@param\n",
        "loss_fn_name = 'bce'  #@param ['hinge', 'bce']\n",
        "\n",
        "data_manager = classifier_data.AgileDataManager(\n",
        "    target_labels=target_labels,\n",
        "    db=db,\n",
        "    train_ratio=train_ratio,\n",
        "    min_eval_examples=1,\n",
        "    batch_size=batch_size,\n",
        "    weak_negatives_batch_size=weak_negatives_batch_size,\n",
        "    rng=np.random.default_rng(seed=5))\n",
        "print('Training for target labels : ')\n",
        "print(data_manager.get_target_labels())\n",
        "linear_classifier, eval_scores = classifier.train_linear_classifier(\n",
        "    data_manager=data_manager,\n",
        "    learning_rate=learning_rate,\n",
        "    weak_neg_weight=weak_neg_weight,\n",
        "    num_train_steps=num_steps,\n",
        ")\n",
        "print('\\n' + '-' * 80)\n",
        "top1 = eval_scores['top1_acc']\n",
        "print(f'top-1      {top1:.3f}')\n",
        "rocauc = eval_scores['roc_auc']\n",
        "print(f'roc_auc    {rocauc:.3f}')\n",
        "cmap = eval_scores['cmap']\n",
        "print(f'cmap       {cmap:.3f}')\n",
        "\n",
        "#@markdown Save the linear classifier to the database folder\n",
        "classifier_filename = 'Biotwang_agile_classifier_v2.pt'  #@param\n",
        "linear_classifier.save(os.path.join(db_path, 'agile_classifier_v2.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3N6dzhetkG1"
      },
      "outputs": [],
      "source": [
        "#@title Review Classifier Results. { vertical-output: true }\n",
        "#@markdown Not only can we examine outputs from the classifier to understand the\n",
        "#@markdown classifier performance, we can also generate additional labels for\n",
        "#@markdown the data and use for another round of training.\n",
        "\n",
        "#@markdown Number of results to find and display.\n",
        "target_label = 'Be_biotwang'  #@param {type:'string'}\n",
        "num_results = 50  #@param\n",
        "\n",
        "target_label_idx = data_manager.get_target_labels().index(target_label)\n",
        "class_query = linear_classifier.beta[:, target_label_idx]\n",
        "bias = linear_classifier.beta_bias[target_label_idx]\n",
        "\n",
        "#@markdown Number of (randomly selected) database entries to search over.\n",
        "sample_size = 1_000  #@param\n",
        "\n",
        "#@markdown Whether to use margin-sampling. If checked, search for examples\n",
        "#@markdown with logits near a particular target score. A typical target score for\n",
        "#@markdown margin sampling is 0.0, which would find examples where the model is more\n",
        "#@markdown uncertain. Higher positive target scores would be more likely to be positive,\n",
        "#@markdown while targeting lower negative scores would produce results that are more likely\n",
        "#@markdown to be negatives.\n",
        "margin_sampling = False  #@param {type: 'boolean'}\n",
        "\n",
        "#@markdown When margin sampling, target this logit.\n",
        "margin_target_score = 0.0  #@param\n",
        "if not margin_sampling:\n",
        "  margin_target_score = None\n",
        "score_fn = score_functions.get_score_fn(\n",
        "    'dot', bias=bias, target_score=margin_target_score)\n",
        "results, all_scores = brutalism.threaded_brute_search(\n",
        "    db, class_query, num_results, score_fn=score_fn,\n",
        "    sample_size=sample_size)\n",
        "\n",
        "# TODO(tomdenton): Better histogram when margin sampling.\n",
        "_ = plt.hist(all_scores, bins=100)\n",
        "hit_scores = [r.sort_score for r in results.search_results]\n",
        "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
        "            color='r', alpha=0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiNoGhyoDF2v"
      },
      "outputs": [],
      "source": [
        "#@title Display results and annotate the output (make sure to save by running the next cell) { vertical-output: true }\n",
        "#@markdown Reminder to click the label button once to mark as a positive label,\n",
        "#@markdown twice for a negative label, and a third time to reset (assumed unlabeled/weak negative).\n",
        "display_results = embedding_display.EmbeddingDisplayGroup.from_search_results(\n",
        "    results, db, sample_rate_hz=audio_loader_sample_rate_hz, frame_rate=100,\n",
        "    audio_loader=audio_filepath_loader)\n",
        "display_results.display(positive_labels=[target_label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMXI3vdfmX48"
      },
      "outputs": [],
      "source": [
        "#@title Save data labels. { vertical-output: true }\n",
        "#@markdown This will save the labels to the database, attached to the embedded examples.\n",
        "\n",
        "prev_lbls, new_lbls = 0, 0\n",
        "for lbl in display_results.harvest_labels(annotator_id):\n",
        "  check = db.insert_label(lbl, skip_duplicates=True)\n",
        "  new_lbls += check\n",
        "  prev_lbls += (1 - check)\n",
        "print('\\nNew labels added: ', new_lbls)\n",
        "print('\\nQuery examples that already existed: ', prev_lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6jOL17UbgMo"
      },
      "outputs": [],
      "source": [
        "#@title Check how many labels of each class exist in the data\n",
        "print('\\nTotal positive labels per class: ', db.get_class_counts())\n",
        "print('\\nTotal negative labels per class: ', db.get_class_counts(label_type = interface.LabelType.NEGATIVE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFRaE-I_jQao"
      },
      "source": [
        "## Repeat and iterate!\n",
        "You can repeat the search process from a given classfier by rerunning the \"Review Classifier Results\" cell (modify for different query settings), reviewing the output, and saving new labels created. We also recommend you try searching for more negative labels (scores around -1 or lower) and/or more labels with scores near 0, not just the highest scoring result which is the default setting.\n",
        "\n",
        "Once you have more labels - try to repeat the classifier training by starting at the beginning of this section starting with \"Classifier Training.\" The model training will now include the labels you just added from the initial classifier results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uzg8vzajM91"
      },
      "source": [
        "# When you are done with your classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19sXdzSCmOwx"
      },
      "outputs": [],
      "source": [
        "#@title Run inference with trained classifier and save results to a .csv { vertical-output: true }\n",
        "#@markdown This will save the results of the classifier to a csv file.\n",
        "\n",
        "output_csv_filepath = '/content/drive/My Drive/noaa_demo/Biotwang_agile_classifier_v2_results.csv' #@param {type:'string'}\n",
        "# @markdown The threshold sets the minimum value for which the results will be saved for a given class.\n",
        "# @markdown If set to 1.0, for example, only scores above 1.0 for a class will be saved to the csv.\n",
        "logit_threshold = 0.0  #@param\n",
        "# @markdown Set labels to a tuple of desired labels if you want to run inference on a\n",
        "# @markdown subset of the labels. If None, then all labels in the data will be included.\n",
        "labels = None  #@param\n",
        "\n",
        "classifier.write_inference_csv(\n",
        "    linear_classifier, db, output_csv_filepath, logit_threshold, labels=labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6vnqMT8eAR8"
      },
      "outputs": [],
      "source": [
        "#@title [Optional] Read the saved csv and examine the results\n",
        "import pandas as pd\n",
        "\n",
        "results_df = pd.read_csv(output_csv_filepath)\n",
        "display(results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1JAZ3WalP7D"
      },
      "outputs": [],
      "source": [
        "results_df.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0pM0sODexHp"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=results_df, x='logits', hue='label', multiple='stack', bins=30)\n",
        "plt.title('Distribution of Logits by Label')\n",
        "plt.xlabel('Logits')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FEpjoVFmLB7"
      },
      "source": [
        "# What to try next?\n",
        "\n",
        "\n",
        "1. Look pick another file from the NOAA data to embed and add to the existing database. The file 'Saipan_A_06_151005_015500.df20.x.flac' is the recording from the time period before the example file we used, and 'Saipan_A_06_151007_163630.df20.x.flac' is the next recording in time.\n",
        "\n",
        "2. Play around with the classifier model training settings.\n",
        "\n",
        "3. Create a new database with embeddings from audio an entirely different fileset - for example your own data saved on a Google Drive Folder.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//gdm/sustainability/perch:perch_notebook",
        "kind": "private"
      },
      "name": "agile_modeling_noaa_demo.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1ePT3-fDB3kA3_T7trthFtu8xTJQWQBoQ",
          "timestamp": 1723499538314
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
