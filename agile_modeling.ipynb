{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEoybFElAON3"
   },
   "source": [
    "# Agile Modeling for Bioacoustics.\n",
    "\n",
    "This notebook provides a workflow for creating custom classifiers for target signals, by first **searching** for training data, and then engaging in an **active learning** loop.\n",
    "\n",
    "We assume that embeddings have been pre-computed using `embed.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gntw2Wq9Atpp"
   },
   "source": [
    "## Configuration and Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "coetZk9xAjCs"
   },
   "outputs": [],
   "source": [
    "#@title Installation. { vertical-output: true }\n",
    "#@markdown Run this notebook in Google Colab by following [this link](https://colab.research.google.com/github/google-research/perch/blob/main/agile_modeling.ipynb).\n",
    "#@markdown\n",
    "#@markdown Run this cell to install the project dependencies.\n",
    "%pip install git+https://github.com/google-research/perch.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "58HLTIdcAzte"
   },
   "outputs": [],
   "source": [
    " #@title Imports. { vertical-output: true }\n",
    "\n",
    "import collections\n",
    "from etils import epath\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from chirp.inference import colab_utils\n",
    "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
    "\n",
    "from chirp import audio_utils\n",
    "from chirp.inference import baw_utils\n",
    "from chirp.inference import interface\n",
    "from chirp.inference import tf_examples\n",
    "from chirp.inference import models\n",
    "from chirp.models import metrics\n",
    "from chirp.taxonomy import namespace\n",
    "from chirp.inference.search import bootstrap\n",
    "from chirp.inference.search import search\n",
    "from chirp.inference.search import display\n",
    "from chirp.inference.classify import classify\n",
    "from chirp.inference.classify import data_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_fMOBVK9A_O1"
   },
   "outputs": [],
   "source": [
    "#@title Basic Configuration. { vertical-output: true }\n",
    "\n",
    "#@markdown Choose what data to work with.\n",
    "#@markdown * For local data (most cases), choose 'filesystem'.\n",
    "#@markdown * For Australian Acoustic Observatory, select 'a2o'.\n",
    "#@markdown This will cause many options (like model_choice) to be overridden.\n",
    "#@markdown Note that you will need an Authentication Token from:\n",
    "#@markdown https://data.acousticobservatory.org/my_account\n",
    "data_source = 'filesystem' #@param['filesystem', 'a2o']\n",
    "baw_auth_token = '' #@param {type:'string'}\n",
    "\n",
    "#@markdown Define the model: Usually perch or birdnet.\n",
    "model_choice = 'perch'  #@param {type:'string'}\n",
    "#@markdown Set the base directory for the project.\n",
    "working_dir = '/tmp/agile'  #@param {type:'string'}\n",
    "\n",
    "#@markdown Set the embedding and labeled data directories.\n",
    "labeled_data_path = epath.Path(working_dir) / 'labeled'\n",
    "custom_classifier_path = epath.Path(working_dir) / 'custom_classifier'\n",
    "\n",
    "#@markdown The embeddings_path should be detected automatically, but can be\n",
    "#@markdown overridden.\n",
    "embeddings_path = ''\n",
    "\n",
    "#@markdown OPTIONAL: Set up separation model.\n",
    "separation_model_key = 'separator_model_tf'  #@param {type:'string'}\n",
    "separation_model_path = ''  #@param {type:'string'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "2c4gJ6S2ETKu"
   },
   "outputs": [],
   "source": [
    "#@title Load Project State and Models. { vertical-output: true }\n",
    "\n",
    "if data_source == 'a2o':\n",
    "  embedding_config = baw_utils.get_a2o_embeddings_config()\n",
    "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_config(\n",
    "      embedding_config=embedding_config,\n",
    "      annotated_path=labeled_data_path,\n",
    "      embeddings_glob = '*/embeddings-*')\n",
    "  embeddings_path = embedding_config.output_dir\n",
    "elif (embeddings_path\n",
    "      or (epath.Path(working_dir) / 'embeddings/config.json').exists()):\n",
    "  if not embeddings_path:\n",
    "    # Use the default embeddings path, as it seems we found a config there.\n",
    "    embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
    "  # Get relevant info from the embedding configuration.\n",
    "  bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_path(\n",
    "      embeddings_path=embeddings_path,\n",
    "      annotated_path=labeled_data_path)\n",
    "  if (bootstrap_config.model_key == 'separate_embed_model'\n",
    "      and not separation_model_path.strip()):\n",
    "    separation_model_key = 'separator_model_tf'\n",
    "    separation_model_path = bootstrap_config.model_config.separator_model_tf_config.model_path\n",
    "else:\n",
    "  raise ValueError('No embedding configuration found.')\n",
    "\n",
    "project_state = bootstrap.BootstrapState(\n",
    "    bootstrap_config, baw_auth_token=baw_auth_token)\n",
    "\n",
    "# Load separation model.\n",
    "if separation_model_path:\n",
    "  separation_config = config_dict.ConfigDict({\n",
    "      'model_path': separation_model_path,\n",
    "      'frame_size': 32000,\n",
    "      'sample_rate': 32000,\n",
    "  })\n",
    "  separator = models.model_class_map()[\n",
    "      separation_model_key].from_config(separation_config)\n",
    "  print('Loaded separator model at {}'.format(separation_model_path))\n",
    "else:\n",
    "  print('No separation model loaded.')\n",
    "  separator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0p0qkxcFSG0"
   },
   "source": [
    "## Search Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eML2kUhuGfZQ"
   },
   "source": [
    "### Query Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "xc1ExwK2GbQx"
   },
   "outputs": [],
   "source": [
    "#@title Load query audio. { vertical-output: true }\n",
    "\n",
    "#@markdown You may specify:\n",
    "#@markdown * an audio filepath (like `/home/me/audio/example.wav`),\n",
    "#@markdown * a Xeno-Canto id (like `xc12345`), or\n",
    "#@markdown * an audio file URL (like\n",
    "#@markdown https://upload.wikimedia.org/wikipedia/commons/7/7c/Turdus_merula_2.ogg).\n",
    "audio_path = 'xc692557'  #@param\n",
    "#@markdown Choose the start time for the audio window within the file.\n",
    "#@markdown We will focus on the model's `window_size_s` seconds of audio,\n",
    "#@markdown starting from `start_s` seconds into the file.\n",
    "start_s = 0  #@param\n",
    "\n",
    "window_s = bootstrap_config.model_config['window_size_s']\n",
    "sample_rate = bootstrap_config.model_config['sample_rate']\n",
    "audio = audio_utils.load_audio(audio_path, sample_rate)\n",
    "\n",
    "# Display the full file.\n",
    "display.plot_audio_melspec(audio, sample_rate)\n",
    "\n",
    "# Display the selected window.\n",
    "print('-' * 80)\n",
    "print('Selected audio window:')\n",
    "st = int(start_s * sample_rate)\n",
    "end = int(st + window_s * sample_rate)\n",
    "if end > audio.shape[0]:\n",
    "  end = audio.shape[0]\n",
    "  st = max([0, int(end - window_s * sample_rate)])\n",
    "audio_window = audio[st:end]\n",
    "display.plot_audio_melspec(audio_window, sample_rate)\n",
    "\n",
    "query_audio = audio_window\n",
    "sep_outputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uELuAcwjGj4h"
   },
   "outputs": [],
   "source": [
    "#@title Separate the target audio window { vertical-output: true }\n",
    "\n",
    "if separator is not None:\n",
    "  sep_outputs = separator.embed(audio_window)\n",
    "\n",
    "  for c in range(sep_outputs.separated_audio.shape[0]):\n",
    "    print(f'Channel {c}')\n",
    "    display.plot_audio_melspec(sep_outputs.separated_audio[c, :], sample_rate)\n",
    "    print('-' * 80)\n",
    "else:\n",
    "  sep_outputs = None\n",
    "  print('No separation model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "atpdag0FGkud"
   },
   "outputs": [],
   "source": [
    "#@title Select the query channel. { vertical-output: true }\n",
    "\n",
    "#@markdown Choose a name for the class.\n",
    "query_label = 'my_class'  #@param\n",
    "#@markdown If you have applied separation, choose a channel.\n",
    "#@markdown Ignored if no separation model is being used.\n",
    "query_channel = 0  #@param\n",
    "\n",
    "if query_channel < 0 or sep_outputs is None:\n",
    "  query_audio = audio_window\n",
    "else:\n",
    "  query_audio = sep_outputs.separated_audio[query_channel].copy()\n",
    "\n",
    "display.plot_audio_melspec(query_audio, sample_rate)\n",
    "\n",
    "outputs = project_state.embedding_model.embed(query_audio)\n",
    "query = outputs.pooled_embeddings('first', 'first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Sx9wlJGo9y"
   },
   "source": [
    "### Execute Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tOV_G29mGm_Z"
   },
   "outputs": [],
   "source": [
    "#@title Run Top-K Search. { vertical-output: true }\n",
    "\n",
    "#@markdown Number of search results to capture.\n",
    "top_k = 50  #@param\n",
    "\n",
    "#@markdown Target distance for search results.\n",
    "#@markdown This lets us try to hone in on a 'classifier boundary' instead of\n",
    "#@markdown just looking at the closest matches.\n",
    "#@markdown Set to 'None' for raw 'best results' search.\n",
    "target_score = None  #@param\n",
    "\n",
    "#@markdown Maximimum Inner-Product (mip) generally gives best results.\n",
    "metric = 'mip'  #@param['euclidean', 'mip', 'cosine']\n",
    "\n",
    "#@markdown If True, produce a fully-random sample of data, ignoring similarity.\n",
    "random_sample = False  #@param\n",
    "\n",
    "ds = project_state.create_embeddings_dataset(shuffle_files=True)\n",
    "results, all_scores = search.search_embeddings_parallel(\n",
    "    ds, query,\n",
    "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    top_k=top_k, target_score=target_score, score_fn=metric,\n",
    "    random_sample=random_sample)\n",
    "\n",
    "# Plot histogram of distances\n",
    "ys, _, _ = plt.hist(all_scores, bins=128, density=True)\n",
    "hit_scores = [r.score for r in results.search_results]\n",
    "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
    "            color='r', alpha=0.5)\n",
    "\n",
    "plt.xlabel(metric)\n",
    "plt.ylabel('density')\n",
    "if target_score is not None:\n",
    "  plt.plot([target_score, target_score], [0.0, np.max(ys)], 'r:')\n",
    "  # Compute the proportion of scores < target_score\n",
    "  hit_percentage = (all_scores < target_score).mean()\n",
    "  print(f'score < target_score percentage : {hit_percentage:5.3f}')\n",
    "min_score = np.min(all_scores)\n",
    "plt.plot([min_score, min_score], [0.0, np.max(ys)], 'g:')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "a8SYu-M2GsZx"
   },
   "outputs": [],
   "source": [
    "#@title Display results. { vertical-output: true }\n",
    "\n",
    "samples_per_page = 25\n",
    "page_state = display.PageState(\n",
    "    np.ceil(len(results.search_results) / samples_per_page))\n",
    "\n",
    "display.display_paged_results(\n",
    "    results, page_state, samples_per_page,\n",
    "    project_state=project_state,\n",
    "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
    "    exclusive_labels=False,\n",
    "    checkbox_labels=[query_label, 'unknown'],\n",
    "    max_workers=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-wi-TOGDGvh5"
   },
   "outputs": [],
   "source": [
    "#@title Write annotated examples. { vertical-output: true }\n",
    "\n",
    "results.write_labeled_data(bootstrap_config.annotated_path,\n",
    "                           project_state.embedding_model.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbZkOXncFYKm"
   },
   "source": [
    "## Active Learning for a Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "NcLdypLeHmss"
   },
   "outputs": [],
   "source": [
    "# @title Load+Embed the Labeled Dataset. { vertical-output: true }\n",
    "\n",
    "#@markdown Time-pooling strategy for audio longer than the model's window size.\n",
    "time_pooling = 'mean'  # @param\n",
    "\n",
    "merged = data_lib.MergedDataset.from_folder_of_folders(\n",
    "    base_dir=labeled_data_path,\n",
    "    embedding_model=project_state.embedding_model,\n",
    "    time_pooling=time_pooling,\n",
    "    load_audio=False,\n",
    "    target_sample_rate=-2,\n",
    "    audio_file_pattern='*',\n",
    "    embedding_config_hash=bootstrap_config.embedding_config_hash(),\n",
    ")\n",
    "\n",
    "# Label distribution\n",
    "lbl_counts = np.sum(merged.data['label_hot'], axis=0)\n",
    "print('num classes :', (lbl_counts > 0).sum())\n",
    "print('mean ex / class :', lbl_counts.sum() / (lbl_counts > 0).sum())\n",
    "print('min ex / class :', (lbl_counts + (lbl_counts == 0) * 1e6).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "7mY5rIiDHoE0"
   },
   "outputs": [],
   "source": [
    "#@title Train small model over embeddings. { vertical-output: true }\n",
    "\n",
    "#@markdown Number of random training examples to choose form each class.\n",
    "#@markdown Set exactly one of `train_ratio` and `train_examples_per_class`.\n",
    "train_ratio = 0.9  #@param\n",
    "train_examples_per_class = None  #@param\n",
    "\n",
    "#@markdown Number of random re-trainings. Allows judging model stability.\n",
    "num_seeds = 3  #@param\n",
    "\n",
    "# Classifier training hyperparams.\n",
    "# These should be good defaults.\n",
    "batch_size = 32\n",
    "num_epochs = 128\n",
    "num_hiddens = -1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "for seed in tqdm.tqdm(range(num_seeds)):\n",
    "  if num_hiddens > 0:\n",
    "    model = classify.get_two_layer_model(\n",
    "        num_hiddens, merged.embedding_dim, merged.num_classes)\n",
    "  else:\n",
    "    model = classify.get_linear_model(\n",
    "        merged.embedding_dim, merged.num_classes)\n",
    "  run_metrics = classify.train_embedding_model(\n",
    "      model, merged, train_ratio, train_examples_per_class,\n",
    "      num_epochs, seed, batch_size, learning_rate)\n",
    "  metrics['acc'].append(run_metrics.top1_accuracy)\n",
    "  metrics['auc_roc'].append(run_metrics.auc_roc)\n",
    "  metrics['cmap'].append(run_metrics.cmap_value)\n",
    "  metrics['maps'].append(run_metrics.class_maps)\n",
    "  metrics['test_logits'].append(run_metrics.test_logits)\n",
    "\n",
    "mean_acc = np.mean(metrics['acc'])\n",
    "mean_auc = np.mean(metrics['auc_roc'])\n",
    "mean_cmap = np.mean(metrics['cmap'])\n",
    "# Merge the test_logits into a single array.\n",
    "test_logits = {\n",
    "    k: np.concatenate([logits[k] for logits in metrics['test_logits']])\n",
    "    for k in metrics['test_logits'][0].keys()\n",
    "}\n",
    "\n",
    "print(f'acc:{mean_acc:5.2f}, auc_roc:{mean_auc:5.2f}, cmap:{mean_cmap:5.2f}')\n",
    "for lbl, auc in zip(merged.labels, run_metrics.class_maps):\n",
    "  if np.isnan(auc):\n",
    "    continue\n",
    "  print(f'\\n{lbl:8s}, auc_roc:{auc:5.2f}')\n",
    "  colab_utils.prstats(f'test_logits({lbl})',\n",
    "                      test_logits[merged.labels.index(lbl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-0Kg43YhH0p3"
   },
   "outputs": [],
   "source": [
    "#@title Run model on target unlabeled data. { vertical-output: true }\n",
    "\n",
    "#@markdown Choose the target class to work with.\n",
    "target_class = 'my_class'  #@param\n",
    "#@markdown Choose a target logit; will display results close to the target.\n",
    "#@markdown Set to None to get the highest-logit examples.\n",
    "target_logit = 0.0  #@param\n",
    "#@markdown Number of results to display.\n",
    "num_results = 50  #@param\n",
    "\n",
    "embeddings_ds = project_state.create_embeddings_dataset(\n",
    "    shuffle_files=True)\n",
    "target_class_idx = merged.labels.index(target_class)\n",
    "results, all_logits = search.classifer_search_embeddings_parallel(\n",
    "    embeddings_classifier=model,\n",
    "    target_index=target_class_idx,\n",
    "    embeddings_dataset=embeddings_ds,\n",
    "    hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    target_score=target_logit,\n",
    "    top_k=num_results\n",
    ")\n",
    "\n",
    "# Plot the histogram of logits.\n",
    "ys, _, _ = plt.hist(all_logits, bins=128, density=True)\n",
    "plt.xlabel(f'{target_class} logit')\n",
    "plt.ylabel('density')\n",
    "# plt.yscale('log')\n",
    "plt.plot([target_logit, target_logit], [0.0, np.max(ys)], 'r:')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "gHI2WJwPH2Wy"
   },
   "outputs": [],
   "source": [
    "#@title Display results for the target label. { vertical-output: true }\n",
    "\n",
    "display_labels = merged.labels\n",
    "\n",
    "#@markdown Specify any extra labels you would like displayed.\n",
    "extra_labels = []  #@param\n",
    "for label in extra_labels:\n",
    "  if label not in merged.labels:\n",
    "    display_labels += (label,)\n",
    "if 'unknown' not in merged.labels:\n",
    "  display_labels += ('unknown',)\n",
    "\n",
    "samples_per_page = 25\n",
    "page_state = display.PageState(\n",
    "    np.ceil(len(results.search_results) / samples_per_page))\n",
    "\n",
    "display.display_paged_results(\n",
    "    results, page_state, samples_per_page,\n",
    "    project_state=project_state,\n",
    "    embedding_sample_rate=project_state.embedding_model.sample_rate,\n",
    "    exclusive_labels=False,\n",
    "    checkbox_labels=display_labels,\n",
    "    max_workers=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mDCcKfBGH4b_"
   },
   "outputs": [],
   "source": [
    "#@title Add selected results to the labeled data. { vertical-output: true }\n",
    "\n",
    "results.write_labeled_data(\n",
    "    bootstrap_config.annotated_path,\n",
    "    project_state.embedding_model.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ZxasEcnhd7kP"
   },
   "outputs": [],
   "source": [
    "#@title Save the Custom Classifier. { vertical-output: true }\n",
    "\n",
    "wrapped_model = interface.LogitsOutputHead(\n",
    "    model_path=custom_classifier_path.as_posix(),\n",
    "    logits_key='logits',\n",
    "    logits_model=model,\n",
    "    class_list=namespace.ClassList('custom', merged.labels),\n",
    ")\n",
    "wrapped_model.save_model(\n",
    "    custom_classifier_path,\n",
    "    embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNw_uivxIJda"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XdpUiaYiIMIp"
   },
   "outputs": [],
   "source": [
    "#@title Write classifier inference CSV. { vertical-output: true }\n",
    "\n",
    "#@markdown This cell writes detections (locations of audio windows where\n",
    "#@markdown the logit was greater than a threshold) to a CSV file.\n",
    "\n",
    "output_filepath = epath.Path(working_dir) / 'inference.csv'  #@param\n",
    "\n",
    "#@markdown Set the default detection thresholds, used for all classes.\n",
    "#@markdown To set per-class detection thresholds, modify the code below.\n",
    "#@markdown Keep in mind that thresholds are on the logit scale, so 0.0\n",
    "#@markdown corresponds to a 50% model confidence.\n",
    "default_threshold = 0.0  #@param\n",
    "if default_threshold is None:\n",
    "  # In this case, all logits are written. This can lead to very large CSV files.\n",
    "  class_thresholds = None\n",
    "else:\n",
    "  class_thresholds = collections.defaultdict(lambda: default_threshold)\n",
    "  # Add any per-class thresholds here.\n",
    "  class_thresholds['my_class'] = 1.0\n",
    "\n",
    "#@markdown Classes to ignore when counting detections.\n",
    "exclude_classes = ['unknown']  #@param\n",
    "\n",
    "#@markdown The `include_classes` list is ignored if empty.\n",
    "#@markdown If non-empty, only scores for these classes will be written.\n",
    "include_classes = []  #@param\n",
    "\n",
    "embeddings_ds = project_state.create_embeddings_dataset(\n",
    "    shuffle_files=True)\n",
    "classify.write_inference_csv(\n",
    "    embeddings_ds=embeddings_ds,\n",
    "    model=model,\n",
    "    labels=merged.labels,\n",
    "    output_filepath=output_filepath,\n",
    "    threshold=class_thresholds,\n",
    "    embedding_hop_size_s=bootstrap_config.embedding_hop_size_s,\n",
    "    include_classes=include_classes,\n",
    "    exclude_classes=exclude_classes)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
