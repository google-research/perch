{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjNsE-YjbCew"
   },
   "source": [
    "# Mass Embedding of Bioacoustic Audio\n",
    "\n",
    "This notebook facilitates pre-computing embeddings of audio data for subsequent\n",
    "use with search, classification, and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avlqyEzpa_rN"
   },
   "source": [
    "## Configuration and Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XPdB8b3qAb6I"
   },
   "outputs": [],
   "source": [
    "#@title Installation. { vertical-output: true }\n",
    "#@markdown Run this notebook in Google Colab by following [this link](https://colab.research.google.com/github/google-research/perch/blob/main/embed_audio.ipynb).\n",
    "#@markdown\n",
    "#@markdown Run this cell to install the project dependencies.\n",
    "%pip install git+https://github.com/google-research/perch.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "3kuA7b5Wap7o"
   },
   "outputs": [],
   "source": [
    "#@title Imports. { vertical-output: true }\n",
    "\n",
    "from etils import epath\n",
    "from ml_collections import config_dict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from chirp.inference import colab_utils\n",
    "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
    "\n",
    "from chirp import audio_utils\n",
    "from chirp.inference import embed_lib\n",
    "from chirp.inference import tf_examples\n",
    "from hoplite.zoo import model_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-l4NL0CAavKl"
   },
   "outputs": [],
   "source": [
    "#@title Basic Configuration. { vertical-output: true }\n",
    "\n",
    "#@markdown Define the model: perch or birdnet are most common for birds.\n",
    "model_choice = 'perch_8'  #@param['perch_8', 'humpback', 'multispecies_whale', 'surfperch', 'birdnet_V2.3']\n",
    "#@markdown Set the base directory for the project.\n",
    "working_dir = '/tmp/agile'  #@param\n",
    "\n",
    "# Set the embedding and labeled data directories.\n",
    "embeddings_path = epath.Path(working_dir) / 'embeddings'\n",
    "labeled_data_path = epath.Path(working_dir) / 'labeled'\n",
    "embeddings_glob = embeddings_path / 'embeddings-*'\n",
    "\n",
    "# OPTIONAL: Set up separation model.\n",
    "separation_model_key = 'separator_model_tf'  #@param\n",
    "separation_model_path = ''  #@param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7W8Rl0ma8Mm"
   },
   "source": [
    "## Embed Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vobyRomeazNr"
   },
   "outputs": [],
   "source": [
    "#@title Embedding Configuration. { vertical-output: true }\n",
    "\n",
    "config = config_dict.ConfigDict()\n",
    "config.embed_fn_config = config_dict.ConfigDict()\n",
    "config.embed_fn_config.model_config = config_dict.ConfigDict()\n",
    "\n",
    "#@markdown IMPORTANT: Select the target audio files.\n",
    "#@markdown source_file_patterns should contain a list of globs of audio files, like:\n",
    "#@markdown ['/home/me/*.wav', '/home/me/other/*.flac']\n",
    "config.source_file_patterns = ['gs://chirp-public-bucket/soundscapes/powdermill/Recording*/*.wav']  #@param\n",
    "config.output_dir = embeddings_path.as_posix()\n",
    "\n",
    "model_key, embedding_dim, model_config = model_configs.get_preset_model_config(\n",
    "    model_choice)\n",
    "config.embed_fn_config.model_key = model_key\n",
    "config.embed_fn_config.model_config = model_config\n",
    "\n",
    "# Only write embeddings to reduce size.\n",
    "config.embed_fn_config.write_embeddings = True\n",
    "config.embed_fn_config.write_logits = False\n",
    "config.embed_fn_config.write_separated_audio = False\n",
    "config.embed_fn_config.write_raw_audio = False\n",
    "\n",
    "#@markdown File sharding automatically splits audio files into one-minute chunks\n",
    "#@markdown for embedding. This limits both system and GPU memory usage,\n",
    "#@markdown especially useful when working with long files (>1 hour).\n",
    "use_file_sharding = True  #@param {type:'boolean'}\n",
    "if use_file_sharding:\n",
    "  config.shard_len_s = 60.0\n",
    "\n",
    "# Number of parent directories to include in the filename.\n",
    "config.embed_fn_config.file_id_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FiWVT22ja1Y0"
   },
   "outputs": [],
   "source": [
    "#@title Set up. { vertical-output: true }\n",
    "\n",
    "# Set up the embedding function, including loading models.\n",
    "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
    "print('\\n\\nLoading model(s)...')\n",
    "embed_fn.setup()\n",
    "\n",
    "# Create output directory and write the configuration.\n",
    "output_dir = epath.Path(config.output_dir)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "embed_lib.maybe_write_config(config, output_dir)\n",
    "\n",
    "# Create SourceInfos.\n",
    "source_infos = embed_lib.create_source_infos(\n",
    "    config.source_file_patterns,\n",
    "    num_shards_per_file=config.get('num_shards_per_file', -1),\n",
    "    shard_len_s=config.get('shard_len_s', -1))\n",
    "print(f'Found {len(source_infos)} source infos.')\n",
    "\n",
    "print('\\n\\nTest-run of model...')\n",
    "window_size_s = config.embed_fn_config.model_config.window_size_s\n",
    "sr = config.embed_fn_config.model_config.sample_rate\n",
    "z = np.zeros([int(sr * window_size_s)], dtype=np.float32)\n",
    "embed_fn.embedding_model.embed(z)\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jf8RVwRwa350"
   },
   "outputs": [],
   "source": [
    "#@title Run embedding. { vertical-output: true }\n",
    "\n",
    "# Uses multiple threads to load audio before embedding.\n",
    "# This tends to be faster, but can fail if any audio files are corrupt.\n",
    "\n",
    "embed_fn.min_audio_s = 1.0\n",
    "record_file = (output_dir / 'embeddings.tfrecord').as_posix()\n",
    "succ, fail = 0, 0\n",
    "\n",
    "existing_embedding_ids = embed_lib.get_existing_source_ids(\n",
    "    output_dir, 'embeddings-*')\n",
    "\n",
    "new_source_infos = embed_lib.get_new_source_infos(\n",
    "    source_infos, existing_embedding_ids, config.embed_fn_config.file_id_depth)\n",
    "\n",
    "print(f'Found {len(existing_embedding_ids)} existing embedding ids. \\n'\n",
    "      f'Processing {len(new_source_infos)} new source infos. ')\n",
    "\n",
    "try:\n",
    "  audio_loader = lambda fp, offset: audio_utils.load_audio_window(\n",
    "      fp, offset, sample_rate=config.embed_fn_config.model_config.sample_rate,\n",
    "      window_size_s=config.get('shard_len_s', -1.0))\n",
    "  audio_iterator = audio_utils.multi_load_audio_window(\n",
    "      filepaths=[s.filepath for s in new_source_infos],\n",
    "      offsets=[s.shard_num * s.shard_len_s for s in new_source_infos],\n",
    "      audio_loader=audio_loader,\n",
    "  )\n",
    "  with tf_examples.EmbeddingsTFRecordMultiWriter(\n",
    "      output_dir=output_dir, num_files=config.get('tf_record_shards', 1)) as file_writer:\n",
    "    for source_info, audio in tqdm.tqdm(\n",
    "        zip(new_source_infos, audio_iterator), total=len(new_source_infos)):\n",
    "      if not embed_fn.validate_audio(source_info, audio):\n",
    "        continue\n",
    "      file_id = source_info.file_id(config.embed_fn_config.file_id_depth)\n",
    "      offset_s = source_info.shard_num * source_info.shard_len_s\n",
    "      example = embed_fn.audio_to_example(file_id, offset_s, audio)\n",
    "      if example is None:\n",
    "        fail += 1\n",
    "        continue\n",
    "      file_writer.write(example.SerializeToString())\n",
    "      succ += 1\n",
    "    file_writer.flush()\n",
    "finally:\n",
    "  del(audio_iterator)\n",
    "print(f'\\n\\nSuccessfully processed {succ} source_infos, failed {fail} times.')\n",
    "\n",
    "fns = [fn for fn in output_dir.glob('embeddings-*')]\n",
    "ds = tf.data.TFRecordDataset(fns)\n",
    "parser = tf_examples.get_example_parser()\n",
    "ds = ds.map(parser)\n",
    "for ex in ds.as_numpy_iterator():\n",
    "  print(ex['filename'])\n",
    "  print(ex['embedding'].shape, flush=True)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "_CVL3zfEmy9h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
